[[2023-01-05]] #LinearAlgebra #GD 

### Review
#### Vector Norms

The *p*-norm of a vector is given by

![[Pasted image 20230105173546.png|200]]

#### Dot Product

![[Pasted image 20230105173732.png|300]]

```ad-note
**Orthogonality**: when two nonzero vectors have a dot product of 0, they are orthogonal to each other.
```

#### Projection
The **orthogonal projection** of v onto the line spanned by nonzero s can be calculated as:

![[Pasted image 20230105174324.png|200]]

```ad-important
**Proof**: We are trying to solve for $(\vec{v}-\vec{p}_\vec{s})\cdot \vec{s}=0$
```

---

#### Gradient
**Properties** for $f, g:\mathbb{R}^n \to \mathbb{R}$ and $k \in \mathbb{R}$
1. $\nabla kf=k\nabla f$
2. $\nabla(f\pm g)=\nabla f \pm \nabla g$
3. Product Rule: $\nabla (fg)=f\nabla g+(\nabla f)g$

#### Chain Rule
Given two differentiable functions,  $g:\mathbb{R}^n \to \mathbb{R}$ and $f:\mathbb{R} \to \mathbb{R}$:

![[Pasted image 20230105180005.png|250]]
